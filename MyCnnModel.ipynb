{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own model\n",
    "This Jupyter notebook is divided into three main parts. The first part under the Markdown [“Google Colab”](#google-colab) contains the necessary preparation and instructions to train the models in Google Colab. The second part under the Markdown [“Local”](#local) contains the necessary preparation and instructions to train the models locally. The third and last part under the Markdown [\"Model\"](#models) contains all models which are functional with the respective preparation in Google Colab as well as locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab\n",
    "The code below is intended for execution in Google Colab. To execute the code in Google Colab you have to comment out the above import for Google Colab and compress the already restructured folder into a zip file and upload “Dataset_Original.zip” to your Google Drive.\n",
    "\n",
    "You can then extract the folder with the ZIP extractor.\n",
    "\n",
    "<img src=\"ImageLib/GoogleDriveZip.jpg\" alt=\"GoogleDrive image\" style=\"width:700px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports to work in Google Colab\n",
    "from google.colab import drive \n",
    "drive.mount('/content/drive') \n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below Code first defines the paths for the training and test data directories. The image size is set to 224x224 pixels and the image shape takes into account three color channels (RGB). The batch size for processing is 32 images.\n",
    "When loading the datasets, the training dataset is split into 75% training data and 25% validation data, while the test dataset contains all images from the test directory. The data sets are randomly mixed, whereby a seed value ensures reproducibility.\n",
    "The class names are extracted from the training dataset and output. A normalization layer is used to scale the image pixels to a range of 0-1, and this normalization is applied to all datasets. Caching and prefetching of the datasets is used to increase efficiency.\n",
    "Finally, data augmentation is defined that includes random augmentations such as horizontal flipping, rotation and zooming to make the model more robust and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for training and test data\n",
    "train_dir = '/content/drive/MyDrive/ML2/Dataset_Original/train'\n",
    "test_dir = '/content/drive/MyDrive/ML2/Dataset_Original/test'\n",
    "\n",
    "# Image sizes and batch size\n",
    "IMAGE_SIZE = (224, 224) \n",
    "IMAGE_SHAPE = IMAGE_SIZE + (3,) \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Load the images from the directories\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=999,\n",
    "    validation_split=0.25,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=999,\n",
    "    validation_split=0.25,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=999\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Klassennamen:\", class_names)\n",
    "\n",
    "\n",
    "# Add standardization layer\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Normalize the datasets\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Prefetching und Caching for efficiency\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Definiere die Datenaugmentierungsschichten\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local\n",
    "\n",
    "This code can be executed locally. It uses the data augmentation script [“dataPreparation.py”](dataPreparation.py). A more detailed description of the script can be found below under the markdown [Data preparation](#data-preparation).\n",
    "\n",
    "Below you will find the necessary imports for the local execution of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports to work local\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from dataPreparation import load_and_prepare_datasets, get_data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Please check that the path to your data set matches.\n",
    "\n",
    "The Code first defines the paths for the training and test data directories. The image size is set to 224x224 pixels and the image shape takes into account three color channels (RGB). The batch size for processing is 32 images.\n",
    "\n",
    "In the function \"load_and_prepare_datasets\" the dataset is loaded, when loading the datasets, the training dataset is split into 75% training data and 25% validation data, while the test dataset contains all images from the test directory. The data sets are randomly mixed, whereby a seed value ensures reproducibility. Afterwards, the class names are extracted from the training dataset. Finally, a normalization layer is used to scale the image pixels to a range of 0-1, and this normalization is applied to all datasets. Caching and prefetching of the datasets is used to increase efficiency.\n",
    "\n",
    "\n",
    "The function \"get_data_augmentation\" includes random augmentations such as horizontal flipping, rotation and zooming to make the model more robust and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1178 files belonging to 3 classes.\n",
      "Using 884 files for training.\n",
      "Found 1178 files belonging to 3 classes.\n",
      "Using 294 files for validation.\n",
      "Found 393 files belonging to 3 classes.\n",
      "Klassennamen: ['gls', 'nlb', 'nls']\n"
     ]
    }
   ],
   "source": [
    "# Directories for training and test data\n",
    "train_dir = 'Dataset_Original/train'\n",
    "test_dir = 'Dataset_Original/test'\n",
    "\n",
    "\n",
    "# Image sizes and batch size\n",
    "IMAGE_SIZE = (224, 224)\n",
    "IMAGE_SHAPE = IMAGE_SIZE + (3,)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load and prepare the dataset\n",
    "train_dataset, validation_dataset, test_dataset, class_names = load_and_prepare_datasets(train_dir, test_dir, IMAGE_SIZE, BATCH_SIZE)\n",
    "\n",
    "# Show class names\n",
    "print(\"Klassennamen:\", class_names)\n",
    "\n",
    "# Get the data augmentation layers\n",
    "data_augmentation = get_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "In this Section you will find 4 Models.\n",
    "\n",
    "[Model 1](#model-1)\n",
    "\n",
    "[Model 2](#model-2)\n",
    "\n",
    "[Model 3](#model-3)\n",
    "\n",
    "[Model 4](#model-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model 1\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=IMAGE_SHAPE),\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "model_1.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model_1.fit(\n",
    "    train_dataset,\n",
    "    epochs=9,\n",
    "    validation_data=validation_dataset\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_1.evaluate(test_dataset, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "\n",
    "# Visualize the training progress\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()), 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics Model 1', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model 2\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=IMAGE_SHAPE),\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # Für Mehrklassenklassifikation\n",
    "])\n",
    "model_2.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model_2.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=validation_dataset\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_2.evaluate(test_dataset, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "\n",
    "# Visualize the training progress\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()), 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics Model 2', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model 3\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=IMAGE_SHAPE),\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_3.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model_3.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=validation_dataset\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_3.evaluate(test_dataset, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "\n",
    "# Visualize the training progress\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()), 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics Model 3', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=IMAGE_SHAPE),\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "model_4.summary()\n",
    "\n",
    "# Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model_4.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='modelLib/modelTraining/best_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model_4.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_4.evaluate(test_dataset, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "\n",
    "# Visualize the training progress\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()), 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics Model 4', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe\n",
    "Use the following code to save the model and class names if you want to use the application with frontend and backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Safe model\n",
    "model_4.save('modelLib/model_4.keras')\n",
    "\n",
    "#Safe class names\n",
    "#with open('modelLib/class_names.txt', 'w') as f:\n",
    "#    for class_name in class_names:\n",
    "#        f.write(\"%s\\n\" % class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
